---
title: "Import System Troubleshooting Guide"
description: "A comprehensive guide to diagnosing and resolving common issues with the event data import system."
---

# Import System Troubleshooting Guide

A comprehensive guide to diagnosing and resolving common issues with the event data import system.

## Quick Diagnosis

### System Health Check

```bash
# Check if services are running
docker compose -f docker-compose.dev.yml ps

# Check API health
curl http://localhost:3000/api/import/health

# Check database connection
pnpm exec payload migrate:status

# Check job queue status
curl http://localhost:3000/api/debug/jobs
```

### Common Symptoms

| Symptom                         | Likely Cause              | Quick Fix                         |
| ------------------------------- | ------------------------- | --------------------------------- |
| File upload fails immediately   | File size/type validation | Check file format and size limits |
| Upload succeeds but no progress | Job queue not processing  | Restart development server        |
| Geocoding not working           | Missing API key           | Configure `GOOGLE_MAPS_API_KEY`   |
| Rate limit errors               | Too many requests         | Wait for rate limit reset         |
| Import stuck at parsing         | File format issues        | Validate CSV/Excel format         |

## File Upload Problems

### File Size Exceeded

**Error**: `"File too large. Maximum size: 10MB"`

**Causes**:

- File exceeds size limits (10MB unauthenticated, 100MB authenticated)
- Incorrect file size calculation

**Solutions**:

1. **Reduce file size**:

   ```bash
   # Split large CSV files
   split -l 1000 large-events.csv events-part-
   ```

2. **Authenticate for higher limits**:

   ```javascript
   // Add authentication header
   fetch("/api/import/upload", {
     method: "POST",
     headers: {
       Authorization: "Bearer your-token",
     },
     body: formData,
   });
   ```

3. **Adjust limits** (development only):
   ```bash
   # In .env.local
   MAX_FILE_SIZE_UNAUTHENTICATED=20971520  # 20MB
   ```

### Unsupported File Type

**Error**: `"Unsupported file type. Allowed types: text/csv, application/vnd.ms-excel, application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"`

**Causes**:

- File has incorrect MIME type
- File extension doesn't match content
- Corrupted file

**Solutions**:

1. **Verify file format**:

   ```bash
   # Check file type
   file events.csv
   # Should show: events.csv: CSV text

   file events.xlsx
   # Should show: events.xlsx: Microsoft Excel 2007+
   ```

2. **Convert file format**:

   ```bash
   # Convert Excel to CSV
   libreoffice --headless --convert-to csv events.xlsx
   ```

3. **Fix MIME type** (if serving files):
   ```javascript
   // Ensure correct MIME type
   const file = new File([csvContent], "events.csv", {
     type: "text/csv",
   });
   ```

### Malformed CSV/Excel Files

**Error**: `"CSV parsing errors: Parse error at row 5"`

**Common Issues**:

- Unescaped quotes in CSV
- Missing headers
- Inconsistent column count
- Special characters encoding

**Solutions**:

1. **Fix CSV formatting**:

   ```csv
   # Correct format
   title,description,date
   "Event with ""quotes""","Description","2024-03-15"

   # Incorrect format
   title,description,date
   Event with "quotes",Description,2024-03-15
   ```

2. **Validate headers**:

   ```csv
   # Required headers (case-insensitive)
   title,date
   # Optional headers
   description,enddate,location,address,url,category,tags
   ```

3. **Check encoding**:

   ```bash
   # Convert to UTF-8
   iconv -f ISO-8859-1 -t UTF-8 events.csv > events-utf8.csv
   ```

4. **Excel-specific issues**:
   - Ensure data is in the first sheet
   - Remove empty rows at the end
   - Check for merged cells (not supported)

### Network Timeouts

**Error**: Network timeout during upload

**Causes**:

- Large file upload
- Slow network connection
- Server timeout configuration

**Solutions**:

1. **Increase timeout** (development):

   ```javascript
   // In Next.js config
   module.exports = {
     api: {
       bodyParser: {
         sizeLimit: "100mb",
       },
       responseLimit: false,
     },
   };
   ```

2. **Implement retry logic**:
   ```javascript
   async function uploadWithRetry(file, catalogId, maxRetries = 3) {
     for (let attempt = 1; attempt <= maxRetries; attempt++) {
       try {
         return await uploadFile(file, catalogId);
       } catch (error) {
         if (attempt === maxRetries) throw error;
         await new Promise((resolve) => setTimeout(resolve, 1000 * attempt));
       }
     }
   }
   ```

## Processing Issues

### Jobs Stuck in Queue

**Symptoms**:

- Import status remains "pending"
- No progress updates
- Jobs visible in database but not processing

**Diagnosis**:

```bash
# Check job queue
curl http://localhost:3000/api/debug/jobs

# Check database for stuck jobs
psql $DATABASE_URL -c "SELECT * FROM payload_jobs WHERE task LIKE '%import%' ORDER BY created_at DESC LIMIT 10;"
```

**Solutions**:

1. **Restart development server**:

   ```bash
   # Stop and restart
   pnpm dev
   ```

2. **Clear stuck jobs**:

   ```sql
   -- Remove failed jobs
   DELETE FROM payload_jobs WHERE status = 'failed' AND task LIKE '%import%';

   -- Reset stuck jobs
   UPDATE payload_jobs SET status = 'queued' WHERE status = 'running' AND task LIKE '%import%';
   ```

3. **Check job handler errors**:
   ```bash
   # Enable debug logging
   DEBUG=payload:jobs pnpm dev
   ```

### Database Connection Problems

**Error**: `"Database connection failed"`

**Causes**:

- PostgreSQL not running
- Incorrect connection string
- Database permissions

**Solutions**:

1. **Check PostgreSQL status**:

   ```bash
   # With Docker
   docker compose -f docker-compose.dev.yml ps postgres

   # Direct connection test
   psql $DATABASE_URL -c "SELECT version();"
   ```

2. **Restart database**:

   ```bash
   # Restart PostgreSQL container
   make down && make up

   # Or reset completely
   make db-reset
   ```

3. **Verify connection string**:
   ```bash
   # Check .env.local
   echo $DATABASE_URL
   # Should be: postgresql://timetiles_user:timetiles_password@localhost:5432/timetiles
   ```

### Memory Issues with Large Files

**Symptoms**:

- Server crashes during processing
- "Out of memory" errors
- Slow processing times

**Solutions**:

1. **Reduce batch size**:

   ```bash
   # In .env.local
   BATCH_SIZE=50
   GEOCODING_BATCH_SIZE=5
   ```

2. **Increase Node.js memory**:

   ```bash
   # In package.json scripts
   "dev": "node --max-old-space-size=4096 node_modules/.bin/next dev"
   ```

3. **Monitor memory usage**:
   ```javascript
   // Add to job handlers
   console.log("Memory usage:", process.memoryUsage());
   ```

### Batch Processing Failures

**Error**: `"Batch processing failed: Invalid data format"`

**Causes**:

- Data validation errors
- Missing required fields
- Date parsing failures

**Solutions**:

1. **Check data validation**:

   ```javascript
   // Validate required fields
   const requiredFields = ["title", "date"];
   const validRows = data.filter((row) =>
     requiredFields.every(
       (field) => row[field] && row[field].toString().trim(),
     ),
   );
   ```

2. **Fix date formats**:

   ```csv
   # Supported formats
   2024-03-15          # ISO format (recommended)
   03/15/2024          # US format
   03-15-2024          # Alternative format
   ```

3. **Handle validation errors gracefully**:
   ```javascript
   // Skip invalid rows instead of failing
   const processedData = rawData
     .map((row) => {
       try {
         return validateAndProcessRow(row);
       } catch (error) {
         console.warn("Skipping invalid row:", row, error.message);
         return null;
       }
     })
     .filter(Boolean);
   ```

## Geocoding Issues

### API Key Configuration

**Error**: `"Geocoding failed: API key not configured"`

**Solutions**:

1. **Configure Google Maps API key**:

   ```bash
   # In .env.local
   GOOGLE_MAPS_API_KEY=your_actual_api_key_here
   ```

2. **Verify API key permissions**:

   - Go to [Google Cloud Console](https://console.cloud.google.com/)
   - Check that Geocoding API is enabled
   - Verify API key restrictions

3. **Test API key**:
   ```bash
   # Test geocoding API
   curl "https://maps.googleapis.com/maps/api/geocode/json?address=1600+Amphitheatre+Parkway,+Mountain+View,+CA&key=YOUR_API_KEY"
   ```

### Rate Limit Exceeded

**Error**: `"Geocoding API rate limit exceeded"`

**Causes**:

- Too many API calls in short time
- Insufficient API quotas
- Billing issues

**Solutions**:

1. **Check API quotas**:

   - Go to Google Cloud Console → APIs & Services → Quotas
   - Verify daily and per-second limits

2. **Increase delay between batches**:

   ```bash
   # In .env.local
   GEOCODING_DELAY_MS=2000  # 2 seconds between batches
   GEOCODING_BATCH_SIZE=5   # Smaller batches
   ```

3. **Enable billing** (if using Google Maps):
   - Ensure billing account is active
   - Check for billing alerts

### Provider Fallback Failures

**Error**: `"All geocoding providers failed"`

**Causes**:

- Both Google Maps and Nominatim failing
- Network connectivity issues
- Invalid addresses

**Solutions**:

1. **Test providers individually**:

   ```javascript
   // Test Google Maps
   const googleResult = await googleGeocoder.geocode(
     "123 Main St, San Francisco, CA",
   );

   // Test Nominatim
   const nominatimResult = await nominatimGeocoder.geocode(
     "123 Main St, San Francisco, CA",
   );
   ```

2. **Check network connectivity**:

   ```bash
   # Test Nominatim
   curl "https://nominatim.openstreetmap.org/search?q=123+Main+St,+San+Francisco,+CA&format=json"
   ```

3. **Improve address quality**:

   ```csv
   # Good addresses
   "123 Main Street, San Francisco, CA 94102, USA"
   "Central Park, New York, NY, USA"

   # Poor addresses
   "Downtown"
   "Near the mall"
   ```

### Cache Corruption

**Symptoms**:

- Inconsistent geocoding results
- Unexpected cache hits/misses
- Performance degradation

**Solutions**:

1. **Clear geocoding cache**:

   ```sql
   -- Clear all cache entries
   DELETE FROM location_cache;

   -- Clear old entries only
   DELETE FROM location_cache WHERE last_used < NOW() - INTERVAL '30 days';
   ```

2. **Rebuild cache**:

   ```bash
   # Re-run geocoding for recent imports
   curl -X POST http://localhost:3000/api/debug/rebuild-cache
   ```

3. **Check cache statistics**:
   ```sql
   SELECT
     provider,
     COUNT(*) as total_entries,
     AVG(hit_count) as avg_hits,
     AVG(confidence) as avg_confidence
   FROM location_cache
   GROUP BY provider;
   ```

## Rate Limiting Issues

### Rate Limit Exceeded

**Error**: `"Rate limit exceeded. Please try again later."`

**Causes**:

- Too many requests from same IP
- Aggressive polling of progress endpoint
- Multiple concurrent uploads

**Solutions**:

1. **Check rate limit status**:

   ```bash
   # Check headers
   curl -I http://localhost:3000/api/import/upload
   # Look for X-RateLimit-* headers
   ```

2. **Implement proper polling**:

   ```javascript
   // Good: Reasonable polling interval
   const pollInterval = setInterval(checkProgress, 2000); // 2 seconds

   // Bad: Aggressive polling
   const pollInterval = setInterval(checkProgress, 100); // 100ms
   ```

3. **Adjust rate limits** (development):
   ```bash
   # In .env.local
   RATE_LIMIT_FILE_UPLOAD=10        # Increase from 5
   RATE_LIMIT_PROGRESS_CHECK=200    # Increase from 100
   ```

### Authentication for Higher Limits

**Issue**: Need higher rate limits for production use

**Solutions**:

1. **Implement authentication**:

   ```javascript
   // Add JWT token to requests
   const response = await fetch("/api/import/upload", {
     method: "POST",
     headers: {
       Authorization: `Bearer ${userToken}`,
     },
     body: formData,
   });
   ```

2. **Configure user-based limits**:
   ```typescript
   // Different limits for authenticated users
   const limits = user ? AUTHENTICATED_LIMITS : UNAUTHENTICATED_LIMITS;
   ```

## Performance Issues

### Slow Processing Times

**Symptoms**:

- Import takes much longer than expected
- Progress updates are slow
- High CPU/memory usage

**Diagnosis**:

```bash
# Monitor system resources
top -p $(pgrep node)

# Check database performance
psql $DATABASE_URL -c "SELECT * FROM pg_stat_activity WHERE state = 'active';"

# Profile job execution
DEBUG=payload:jobs pnpm dev
```

**Solutions**:

1. **Optimize batch sizes**:

   ```bash
   # Experiment with different sizes
   BATCH_SIZE=200           # Larger batches for CPU-bound tasks
   GEOCODING_BATCH_SIZE=20  # Larger batches if API allows
   ```

2. **Database optimization**:

   ```sql
   -- Add indexes for common queries
   CREATE INDEX CONCURRENTLY idx_imports_status_stage ON imports(status, processing_stage);
   CREATE INDEX CONCURRENTLY idx_events_import_geocoding ON events(import_id) WHERE (geocoding->>'needsGeocoding')::boolean = true;
   ```

3. **Reduce geocoding delays**:
   ```bash
   # If API allows, reduce delay
   GEOCODING_DELAY_MS=500  # Reduce from 1000ms
   ```

### Memory Usage Optimization

**Issue**: High memory consumption during processing

**Solutions**:

1. **Stream processing for large files**:

   ```javascript
   // Instead of loading entire file
   const data = JSON.parse(fs.readFileSync(filePath));

   // Use streaming
   const stream = fs.createReadStream(filePath);
   const parser = Papa.parse(Papa.NODE_STREAM_INPUT, {
     header: true,
     step: (results) => processRow(results.data),
   });
   ```

2. **Garbage collection**:

   ```javascript
   // Force garbage collection after batches
   if (global.gc) {
     global.gc();
   }
   ```

3. **Monitor memory leaks**:
   ```javascript
   // Add memory monitoring
   setInterval(() => {
     const usage = process.memoryUsage();
     console.log("Memory usage:", {
       rss: Math.round(usage.rss / 1024 / 1024) + "MB",
       heapUsed: Math.round(usage.heapUsed / 1024 / 1024) + "MB",
     });
   }, 30000);
   ```

### Database Query Optimization

**Issue**: Slow database queries during import

**Solutions**:

1. **Optimize import queries**:

   ```typescript
   // Use select to limit fields
   const importRecord = await payload.findByID({
     collection: "imports",
     id: importId,
     select: {
       status: true,
       progress: true,
       batchInfo: true,
     },
   });
   ```

2. **Batch database operations**:

   ```typescript
   // Instead of individual creates
   for (const eventData of events) {
     await payload.create({ collection: "events", data: eventData });
   }

   // Use batch operations
   await payload.create({
     collection: "events",
     data: events, // Array of events
   });
   ```

3. **Connection pooling**:
   ```bash
   # In DATABASE_URL
   DATABASE_URL=postgresql://user:pass@localhost:5432/db?max_connections=20
   ```

## Error Code Reference

### HTTP Status Codes

| Code | Meaning               | Common Causes                       | Solutions                    |
| ---- | --------------------- | ----------------------------------- | ---------------------------- |
| 400  | Bad Request           | Invalid file, missing catalogId     | Validate input data          |
| 404  | Not Found             | Invalid catalogId, import not found | Check IDs exist              |
| 413  | Payload Too Large     | File exceeds size limit             | Reduce file size             |
| 429  | Too Many Requests     | Rate limit exceeded                 | Implement retry with backoff |
| 500  | Internal Server Error | Database issues, job failures       | Check logs and database      |

### Import Status Codes

| Status       | Description               | Next Steps                   |
| ------------ | ------------------------- | ---------------------------- |
| `pending`    | Queued for processing     | Wait for processing to start |
| `processing` | Currently being processed | Monitor progress             |
| `completed`  | Successfully finished     | Import is ready              |
| `failed`     | Processing failed         | Check error logs             |

### Processing Stages

| Stage            | Description                         | Common Issues              |
| ---------------- | ----------------------------------- | -------------------------- |
| `file-parsing`   | Reading and validating file         | File format errors         |
| `row-processing` | Processing data rows                | Data validation errors     |
| `geocoding`      | Converting addresses to coordinates | API key issues             |
| `event-creation` | Creating database records           | Database connection issues |
| `completed`      | All processing finished             | None                       |

## Monitoring and Maintenance

### Health Monitoring

```bash
# Create monitoring script
#!/bin/bash
# monitor-imports.sh

echo "=== Import System Health Check ==="
echo "Date: $(date)"
echo

# Check database
echo "Database Status:"
psql $DATABASE_URL -c "SELECT COUNT(*) as active_imports FROM imports WHERE status = 'processing';" 2>/dev/null || echo "Database connection failed"

# Check recent imports
echo -e "\nRecent Imports:"
psql $DATABASE_URL -c "SELECT id, status, processing_stage, created_at FROM imports ORDER BY created_at DESC LIMIT 5;" 2>/dev/null

# Check job queue
echo -e "\nJob Queue:"
psql $DATABASE_URL -c "SELECT task, status, COUNT(*) FROM payload_jobs WHERE task LIKE '%import%' GROUP BY task, status;" 2>/dev/null

# Check geocoding cache
echo -e "\nGeocoding Cache:"
psql $DATABASE_URL -c "SELECT provider, COUNT(*) as entries, AVG(hit_count) as avg_hits FROM location_cache GROUP BY provider;" 2>/dev/null
```

### Maintenance Tasks

```bash
# Weekly maintenance script
#!/bin/bash
# maintenance.sh

echo "Running weekly maintenance..."

# Clean up old cache entries
psql $DATABASE_URL -c "DELETE FROM location_cache WHERE hit_count < 3 AND last_used < NOW() - INTERVAL '90 days';"

# Clean up completed imports older than 30 days
psql $DATABASE_URL -c "DELETE FROM imports WHERE status = 'completed' AND created_at < NOW() - INTERVAL '30 days';"

# Clean up failed jobs
psql $DATABASE_URL -c "DELETE FROM payload_jobs WHERE status = 'failed' AND created_at < NOW() - INTERVAL '7 days';"

# Vacuum database
psql $DATABASE_URL -c "VACUUM ANALYZE;"

echo "Maintenance completed."
```

### Log Analysis

```bash
# Analyze import logs
grep "Import" logs/app.log | grep -E "(failed|error)" | tail -20

# Check geocoding performance
grep "Geocoding" logs/app.log | grep -E "(timeout|failed)" | wc -l

# Monitor memory usage
grep "Memory usage" logs/app.log | tail -10
```

## Getting Help

### Debug Information to Collect

When reporting issues, include:

1. **System information**:

   ```bash
   node --version
   pnpm --version
   docker --version
   ```

2. **Environment details**:

   ```bash
   echo "NODE_ENV: $NODE_ENV"
   echo "Database URL: ${DATABASE_URL%@*}@***" # Hide credentials
   ```

3. **Error logs**:

   ```bash
   # Last 50 lines of logs
   tail -50 logs/app.log

   # Import-specific errors
   grep -A 5 -B 5 "import.*error" logs/app.log
   ```

4. **Import details**:
   ```sql
   SELECT * FROM imports WHERE id = 'your-import-id';
   ```

### Common Support Scenarios

1. **"My import is stuck"**:

   - Provide import ID
   - Check job queue status
   - Include recent logs

2. **"Geocoding isn't working"**:

   - Verify API key configuration
   - Test with sample addresses
   - Check API quotas

3. **"Files won't upload"**:
   - Check file size and format
   - Verify rate limits
   - Test with sample file

### Community Resources

- **GitHub Issues**: Report bugs and feature requests
- **Documentation**: Check latest updates
- **Sample Files**: Use provided test data
- **Test Suite**: Run integration tests

---

For more information, see:

- [Import System Overview](../features/import-system)
- [API Documentation](../api/import)
- [Development Guide](../development/import-system)
