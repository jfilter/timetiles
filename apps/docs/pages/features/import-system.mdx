---
title: "Event Data Import System"
description: "A comprehensive event data import system built with Payload CMS, featuring file upload, background processing, real-time progress tracking, and automatic geocoding."
---

# Event Data Import System

A comprehensive event data import system built with Payload CMS, featuring file upload, background processing, real-time progress tracking, and automatic geocoding.

## Overview

The TimeTiles Event Data Import System provides a comprehensive solution for importing event data from CSV and Excel files with automatic geocoding and real-time progress tracking. The system is designed to handle large datasets efficiently while providing a user-friendly experience for both end users and developers. It supports public access with rate limiting, making it accessible to unauthenticated users while maintaining system stability.

## Features

### Core Functionality

- **üìÅ Multi-format Support**: CSV and Excel files (.xlsx, .xls)
- **üåç Automatic Geocoding**: Google Maps API with OpenStreetMap fallback
- **‚ö° Real-time Progress**: Live updates with detailed processing stages
- **üîì Public Access**: Full functionality for unauthenticated users
- **üöÄ Batch Processing**: Efficient handling of large datasets
- **üíæ Smart Caching**: Intelligent geocoding cache to reduce API calls
- **üõ°Ô∏è Rate Limiting**: Built-in protection against abuse
- **üìä Progress Tracking**: Detailed statistics and time estimates
- **üîí Error Handling**: Comprehensive error tracking and reporting

### User Experience

- **Unauthenticated Access**: Full functionality for anonymous users with rate limits
- **Real-time UI**: Live progress updates with stage-specific information
- **File Validation**: Client and server-side file type and size validation
- **Responsive Design**: Mobile-friendly interface

## Quick Start

### Web Interface

1. Navigate to `/import` in your TimeTiles application
2. Select your CSV or Excel file
3. Enter the target catalog ID
4. Click "Upload & Process"
5. Monitor real-time progress updates

### API Usage

```bash
# Upload a file
curl -X POST http://localhost:3000/api/import/upload \
  -F "file=@events.csv" \
  -F "catalogId=your-catalog-id"

# Track progress
curl http://localhost:3000/api/import/{importId}/progress
```

## File Format Requirements

### Required Fields

All uploaded files must contain these fields:

- **title**: Event name/title (string, required)
- **date**: Event date (string, required, multiple formats supported)

### Optional Fields

- **description**: Event description (string)
- **enddate**: Event end date (string, same formats as date)
- **location**: Venue name (string)
- **address**: Full address for geocoding (string)
- **url**: Event website URL (string)
- **category**: Event category (string)
- **tags**: Comma-separated tags (string)

### Supported Date Formats

- ISO 8601: `2024-03-15`
- US Format: `03/15/2024`
- Alternative: `03-15-2024`

### Sample CSV Format

```csv
title,description,date,enddate,location,address,url,category,tags
"Tech Conference 2024","Annual technology conference","2024-03-15","2024-03-17","Convention Center","123 Main St, San Francisco, CA 94102","https://techconf2024.com","Technology","tech,conference,networking"
"Art Gallery Opening","Contemporary art exhibition","2024-03-20",,"Modern Art Gallery","456 Art Ave, New York, NY 10001","https://modernart.gallery","Arts","art,gallery,exhibition"
```

### Excel Format

- **Single Sheet**: Data should be in the first sheet
- **Multi-Sheet**: Each sheet is treated as a separate data catalog
- **Headers**: First row should contain field names
- **Data Types**: All data is converted to strings during processing

## Processing Stages

The import system processes files through several stages:

1. **File Parsing** - Validates and parses the uploaded file
2. **Batch Processing** - Processes data in configurable batches
3. **Event Creation** - Creates event records in the database
4. **Geocoding** - Converts addresses to coordinates
5. **Completion** - Finalizes the import process

## Rate Limits and File Sizes

### Unauthenticated Users

- **File Uploads**: 5 per hour
- **Progress Checks**: 100 per hour
- **Max File Size**: 10MB

### Authenticated Users

- **File Uploads**: No limits (or much higher limits)
- **Progress Checks**: No limits
- **Max File Size**: 100MB

## Performance

The system is optimized for various file sizes:

| File Size | Rows   | Processing Time | Geocoding Time | Total Time |
| --------- | ------ | --------------- | -------------- | ---------- |
| < 1MB     | 1,000  | 30s             | 2min           | 2.5min     |
| 5MB       | 5,000  | 2min            | 8min           | 10min      |
| 10MB      | 10,000 | 4min            | 15min          | 19min      |
| 50MB      | 50,000 | 15min           | 1.2hr          | 1.4hr      |

_Processing times include geocoding and vary based on address quality and system load._

## Architecture

### Collections

#### Imports Collection (`/lib/collections/Imports.ts`)

Tracks import jobs with comprehensive metadata:

- File information (name, size, type)
- Processing status and stage
- Progress tracking (rows processed, geocoded, created)
- Batch processing information
- Geocoding statistics
- Error tracking
- Rate limiting information
- Job history

#### Events Collection (`/lib/collections/Events.ts`)

Stores imported events with geocoding metadata:

- Basic event information (title, description, dates)
- Location data (coordinates, addresses)
- Geocoding metadata (provider, confidence, timestamps)
- Import tracking (source import ID)

#### LocationCache Collection (`/lib/collections/LocationCache.ts`)

Caches geocoding results for efficiency:

- Address normalization
- Provider information (Google Maps, Nominatim)
- Confidence scores and hit counts
- Usage tracking and cleanup

### Services

#### GeocodingService (`/lib/services/geocoding/GeocodingService.ts`)

Handles address geocoding with multiple providers:

- **Primary**: Google Maps Geocoding API
- **Fallback**: OpenStreetMap Nominatim
- **Features**: Caching, batch processing, confidence scoring, rate limiting

#### RateLimitService (`/lib/services/RateLimitService.ts`)

Manages rate limiting for unauthenticated users:

- In-memory rate limiting with configurable windows
- Per-endpoint rate limits
- Automatic cleanup of expired entries
- HTTP header integration

### Job System

#### File Parsing Job (`file-parsing`)

- Parses CSV/Excel files
- Validates required fields
- Calculates total row count
- Queues batch processing jobs

#### Batch Processing Job (`batch-processing`)

- Processes data in configurable batches (default: 100 rows)
- Normalizes and validates data
- Queues event creation jobs

#### Event Creation Job (`event-creation`)

- Creates event records in database
- Handles validation errors gracefully
- Queues geocoding jobs for events with addresses

#### Geocoding Batch Job (`geocoding-batch`)

- Geocodes event addresses in batches
- Updates events with coordinates and metadata
- Handles provider fallbacks and caching

## Geocoding Features

### Provider Fallback

1. **Primary**: Google Maps Geocoding API (high accuracy)
2. **Fallback**: OpenStreetMap Nominatim (free, no API key required)

### Intelligent Caching

- Results cached to reduce API calls
- Cache key is normalized address
- Cache entries include confidence scores and hit counts
- Automatic cleanup of unused entries (90+ days old, <3 hits)

### Address Quality

The system provides confidence scores for geocoded addresses:

- **0.9+**: High confidence (exact address match)
- **0.7-0.9**: Good confidence (street-level match)
- **0.5-0.7**: Medium confidence (city-level match)
- **0.3-0.5**: Low confidence (region-level match)
- **<0.3**: Very low confidence (may be rejected)

## Security Features

### Input Validation

- File type validation (MIME type checking)
- File size limits
- Required field validation
- Data sanitization during processing

### Rate Limiting

- IP-based rate limiting for unauthenticated users
- Session-based tracking
- Automatic blocking for abuse
- Configurable limits per endpoint

### Data Protection

- Uploaded files are cleaned up after processing
- No persistent storage of raw file data
- SQL injection prevention
- XSS protection in frontend components

## Configuration

### Environment Variables

```bash
# Required for Google Maps geocoding
GOOGLE_MAPS_API_KEY=your_google_maps_api_key

# Database configuration
DATABASE_URL=postgresql://user:password@localhost:5432/database

# Payload configuration
PAYLOAD_SECRET=your_payload_secret
NEXT_PUBLIC_PAYLOAD_URL=http://localhost:3000
```

### Rate Limits (Configurable)

```typescript
export const RATE_LIMITS = {
  FILE_UPLOAD: {
    limit: 5, // 5 uploads per hour
    windowMs: 60 * 60 * 1000, // 1 hour
  },
  PROGRESS_CHECK: {
    limit: 100, // 100 checks per hour
    windowMs: 60 * 60 * 1000, // 1 hour
  },
  API_GENERAL: {
    limit: 50, // 50 requests per hour
    windowMs: 60 * 60 * 1000, // 1 hour
  },
};
```

## Integration Examples

### JavaScript/React

```javascript
import { useState } from "react";

function ImportComponent() {
  const [progress, setProgress] = useState(null);

  const handleUpload = async (file, catalogId) => {
    const formData = new FormData();
    formData.append("file", file);
    formData.append("catalogId", catalogId);

    const response = await fetch("/api/import/upload", {
      method: "POST",
      body: formData,
    });

    const result = await response.json();

    if (result.success) {
      trackProgress(result.importId);
    }
  };

  const trackProgress = (importId) => {
    const poll = async () => {
      const response = await fetch(`/api/import/${importId}/progress`);
      const progressData = await response.json();

      setProgress(progressData);

      if (progressData.status === "completed") {
        console.log("Import completed!");
      } else if (progressData.status !== "failed") {
        setTimeout(poll, 2000);
      }
    };

    poll();
  };

  return (
    <div>
      {/* Upload UI */}
      {progress && (
        <div>
          <p>Status: {progress.status}</p>
          <p>Progress: {progress.stageProgress.percentage}%</p>
          <p>Stage: {progress.stageProgress.stage}</p>
        </div>
      )}
    </div>
  );
}
```

### Python

```python
import requests
import time

def import_events(file_path, catalog_id):
    # Upload file
    with open(file_path, 'rb') as f:
        files = {'file': f}
        data = {'catalogId': catalog_id}

        response = requests.post(
            'http://localhost:3000/api/import/upload',
            files=files,
            data=data
        )

    result = response.json()
    import_id = result['importId']

    # Track progress
    while True:
        progress_response = requests.get(
            f'http://localhost:3000/api/import/{import_id}/progress'
        )
        progress = progress_response.json()

        print(f"Progress: {progress['stageProgress']['percentage']}% - {progress['stageProgress']['stage']}")

        if progress['status'] in ['completed', 'failed']:
            break

        time.sleep(2)

    return progress

# Usage
result = import_events('events.csv', 'catalog_123')
```

## Usage

### Basic Import Flow

1. User uploads CSV/Excel file via `/import` page
2. File is validated and saved to disk
3. Import record is created with initial metadata
4. File parsing job is queued
5. Data is processed in batches
6. Events are created in database
7. Addresses are geocoded automatically
8. Progress is tracked and displayed in real-time

### Frontend Integration

```typescript
// Upload file
const formData = new FormData();
formData.append("file", file);
formData.append("catalogId", catalogId);

const response = await fetch("/api/import/upload", {
  method: "POST",
  body: formData,
});

// Track progress
const progressResponse = await fetch(`/api/import/${importId}/progress`);
const progress = await progressResponse.json();
```

## Testing

### Sample Data

Use the provided sample CSV file at `/sample-data/events-sample.csv` for testing.

### Test Scenarios

1. **Basic Import**: Upload sample CSV with valid data
2. **Large File**: Test batch processing with 1000+ rows
3. **Geocoding**: Include addresses to test geocoding functionality
4. **Error Handling**: Upload file with missing required fields
5. **Rate Limiting**: Test multiple uploads to trigger rate limits

## Error Handling

Common error scenarios and solutions:

### File Upload Errors

- **File too large**: Reduce file size or authenticate for higher limits
- **Unsupported format**: Use CSV or Excel formats
- **Rate limit exceeded**: Wait for rate limit reset

### Processing Errors

- **Invalid data**: Check required fields (title, date)
- **Geocoding failures**: Verify API key configuration
- **Jobs stuck**: Restart development server

### Performance Issues

- **Slow processing**: Reduce batch size or optimize addresses
- **Memory issues**: Process smaller files or increase system resources

## Monitoring and Maintenance

### Geocoding Cache Cleanup

The system automatically cleans up old cache entries:

- Entries with < 3 hits and > 90 days old are removed
- Cleanup runs automatically via the GeocodingService

### Error Tracking

All errors are logged with:

- Timestamp and error message
- Import ID and batch information
- Stack traces for debugging

### Performance Considerations

- Batch size is configurable (default: 100 rows)
- Geocoding includes rate limiting between batches
- Database queries are optimized for large datasets
- File cleanup after processing

## Future Enhancements

### Planned Features

- **Authentication Integration**: Higher limits for authenticated users
- **Advanced Validation**: Custom validation rules per catalog
- **Data Transformation**: Field mapping and transformation rules
- **Webhook Integration**: Notifications on import completion
- **Analytics Dashboard**: Import statistics and performance metrics
- **Bulk Operations**: Mass import management tools

### Performance Optimizations

- **Redis Integration**: Distributed rate limiting and caching
- **Queue Optimization**: Priority queues and job scheduling
- **Database Indexing**: Optimized queries for large datasets
- **CDN Integration**: File upload to cloud storage

## Documentation Links

For detailed technical information:

- **[API Reference](../api/import)** - Detailed API documentation with examples
- **[Development Guide](../development/import-system)** - Setup, testing, and contribution guidelines
- **[Troubleshooting Guide](../guides/troubleshooting)** - Common issues and solutions

### Quick Links

- **Sample Data**: `apps/web/sample-data/events-sample.csv`
- **Import UI**: `/import` (when running the application)
- **API Endpoints**: `/api/import/upload` and `/api/import/[importId]/progress`

## Support and Contributing

### Getting Help

- Check the [Troubleshooting Guide](../guides/troubleshooting) for common issues
- Review the [API Documentation](../api/import) for integration questions
- Use the sample data files for testing

### Contributing

- See the [Development Guide](../development/import-system) for setup instructions
- All contributions should include comprehensive tests
- Follow the existing code style and documentation standards

---

The Event Data Import System is a powerful tool for efficiently importing and processing event data with automatic geocoding and real-time progress tracking. Whether you're uploading files through the web interface or integrating programmatically via the API, the system provides a robust and user-friendly experience.
